{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361fa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "import requests\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad810e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "video_path = Path(\n",
    "    \"SnapInsta.to_AQP6pQCnuGr0gNz8bAkvNgHiq7qFRMTy59yCjbavpPz65WHft-sZdKnE11g2mOrIa-Lpg3_hXJISrqtelf8IhPc_3xQNVHkwY9NBewU.mp4\"\n",
    ")\n",
    "# video_path = Path(\"AISummit.mp4\")\n",
    "am_url = \"http://localhost:8080/predictions/acoustic_model\"\n",
    "decoder_url = \"http://localhost:8080/predictions/asr_decoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e437c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wav_content_to_nparray(wav_content):\n",
    "    \"\"\"Read wav file and extract data\n",
    "\n",
    "    Take wav file content and extract wav data.\n",
    "    \"\"\"\n",
    "    wav_content = io.BytesIO(wav_content)\n",
    "    data, sr = sf.read(wav_content, dtype=np.int16)\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_np_array_to_wav_file_bytes(np_array, fs):\n",
    "    in_memory_file = io.BytesIO()\n",
    "    sf.write(in_memory_file, np_array, fs, format=\"WAV\")\n",
    "    in_memory_file.seek(0)\n",
    "    return in_memory_file.read()\n",
    "\n",
    "\n",
    "def vad(av_content):\n",
    "    args = (\n",
    "        ffmpeg.input(\"pipe:\")\n",
    "        .output(\n",
    "            \"pipe:\",\n",
    "            format=\"wav\",\n",
    "            acodec=\"pcm_s16le\",\n",
    "            ac=1,\n",
    "            ar=sample_rate,\n",
    "        )\n",
    "        .get_args()\n",
    "    )\n",
    "    ffmpeg_process = subprocess.Popen(\n",
    "        [\"ffmpeg\"] + args,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )\n",
    "    wav_content = ffmpeg_process.communicate(input=av_content)[0]\n",
    "    ffmpeg_process.kill()\n",
    "    wav_array = convert_wav_content_to_nparray(wav_content)\n",
    "    duration = len(wav_array) / sample_rate\n",
    "    return wav_array, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506f99b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "av_content = video_path.read_bytes()\n",
    "wav_array, duration = vad(av_content)\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3e604f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input Got: 2 Expected: 3 Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mstreamsad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAD\n\u001b[32m      3\u001b[39m sad = SAD()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m segments = \u001b[43msad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Print the detected segments\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(segments)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/streamsad/sad.py:63\u001b[39m, in \u001b[36mSAD.__call__\u001b[39m\u001b[34m(self, audio_array)\u001b[39m\n\u001b[32m     60\u001b[39m spect = \u001b[38;5;28mself\u001b[39m.feature_extractor(tmp_audio_tensor)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m raw_output, \u001b[38;5;28mself\u001b[39m.state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mort_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m sad_probs = raw_output[\u001b[32m0\u001b[39m, :, \u001b[32m1\u001b[39m]\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Apply smoothing and return speech segments\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:266\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    264\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mInvalidArgument\u001b[39m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: input Got: 2 Expected: 3 Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "from streamsad import SAD\n",
    "\n",
    "sad = SAD()\n",
    "segments = sad(wav_array)\n",
    "\n",
    "# Print the detected segments\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feca5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in segments:\n",
    "    # crop audio segment\n",
    "    start_sample = int(segment[\"start\"] * sample_rate)\n",
    "    end_sample = int(segment[\"end\"] * sample_rate)\n",
    "    segment_wav_array = wav_array[start_sample:end_sample]\n",
    "    data = convert_np_array_to_wav_file_bytes(segment_wav_array, sample_rate)\n",
    "    # ASR\n",
    "    am_result = requests.get(am_url, data=data)\n",
    "    asr_result = requests.get(decoder_url, data=am_result.content)\n",
    "    segment[\"text\"] = asr_result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in segments:\n",
    "    print(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"Format seconds to SRT time: HH:MM:SS,mmm\"\"\"\n",
    "    total_ms = int(round(seconds * 1000))\n",
    "    ms = total_ms % 1000\n",
    "    s = total_ms // 1000\n",
    "    hours = s // 3600\n",
    "    minutes = (s % 3600) // 60\n",
    "    seconds = s % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{ms:03d}\"\n",
    "\n",
    "\n",
    "# ensure segments are in order\n",
    "sorted_segments = sorted(segments, key=lambda seg: seg.get(\"start\", 0.0))\n",
    "\n",
    "srt_lines = []\n",
    "idx = 1\n",
    "for seg in sorted_segments:\n",
    "    text = seg.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        # skip empty segments (no subtitle text)\n",
    "        continue\n",
    "    start_ts = _format_timestamp(seg[\"start\"])\n",
    "    end_ts = _format_timestamp(seg[\"end\"])\n",
    "    srt_lines.append(f\"{idx}\")\n",
    "    srt_lines.append(f\"{start_ts} --> {end_ts}\")\n",
    "    srt_lines.append(text)\n",
    "    srt_lines.append(\"\")  # blank line between entries\n",
    "    idx += 1\n",
    "\n",
    "srt_content = \"\\n\".join(srt_lines)\n",
    "\n",
    "srt_path = video_path.with_suffix(\".srt\")\n",
    "srt_path.write_text(srt_content, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote {idx-1} subtitles to {srt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f454954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
